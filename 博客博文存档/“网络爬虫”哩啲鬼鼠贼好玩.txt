“网络爬虫”哩啲鬼鼠贼好玩
作者：温毛 • 2018-01-14 22:39:26
最近Tatsumi在整理之前的爬虫笔记，发现网络爬虫的话还是比较大的一块领域，涉及到的东西也比较多。之前Tatsumi都是比较的零散的实现一些简单的爬虫小项目，并没有比较系统的整理整个知识架构，所以打算在这里完整的梳理一版爬虫的整体流程与解决思路。

所谓的爬虫，简单的来说这里就是使用编程语言（Python等）或一些第三软件（如八爪鱼，火车头）实现对网站信息的自动搜集与下载。爬虫项目的规模有大有小，像是百度或Google等搜索引擎网站实际上就是一种大规模的爬虫，通过搜集全网的信息再结合自身的关键词推荐算法实现的。而像Tatsumi这种刚毕业的数据科学小白要想获取有用的数据来实现各种算法与搭建自己的知识架构，自己实现爬虫是最好的解决方式了。通过爬虫可以获取网站上公开的数据并对数据进行分析来达到学习的目的，这种自给自足的方式对于资源比较缺乏的数据萌新来说是非常重要的。要比较好的实现和掌握爬虫最好是懂或了解一些基本的网络请求方式，如果懂前端的知识的话就更容易上手了，毕竟爬虫可以看成是是前端的逆操作而已。



下面Tatsumi就分6个板块给整理下爬虫的流程：

1.网站的请求
一般访问网站的话会有两种常用的方式，一种是GET请求一种是POST请求。

简单的说GET请求就是直接向网站服务去拿数据，就是白吃白喝，直接伸手要数据，那个不需要你登录就能看到网页内容的网站一般来说都是GET请求，您现在访问Tatsumi的个人博客也是一个GET请求。

POST请求就是先给服务器发送一些数据，服务器获取你的数据或才会返回页面信息，这种网站使用的就是POST请求。POST请求一般用在登陆或信息提交的地方，像是微博登陆等等。

Python可以使用requests结合urllib库来实现网络请求



另外，有些网站需要登陆才能获取到信息，我们可以在发送请求时把Cookies信息也发送过去，模拟登陆的状态。

如皋有些网站因为你的多次频繁请求，把你的ip地址拉入了黑名单，你就可以通过更换ip地址（代理）的方式解决这个问题。推荐购买代理，网络上的免费代理一般都不太稳定，会经常出现各种问题。

这里值得提的是，我们实现网络爬虫要带着善意，注意适度，不要把别人的服务器给搞垮，毕竟把别人的服务器搞垮对自己也是没有意义的，大家好才是真的好嘛。

2.数据解析
第二步就是网络解析，当我们通过网络请求获取到网页的html时，一般来说我们只需要html中有价值的数据或资料，能在杂乱的html中提取需要的或有用的信息就是网络解析。

网络解析一般有3种方法：CSS选择器（BeautifulSoup）、Xpath（lxml）和正则表达式（re）

解析工具	解析速度	使用难度
BeautifulSoup	最慢	最简单
lxml	快	简单
正则	最快	最难
BeautifulSoup的原理是把整个html代码转化成一种树结构，然后通过解析CSS选择器来选择需要的元素，因为要解析整个DOM树，所以时间和内存开销都比较大，执行效率会没有这么高，Python中使用BeautifulSoup4库来实现。

lxml是一个HTML/XML的解析器，主要的功能是能解析和提取 HTML/XML 数据，使用的语法是Xpath语法，在Python中lxml是用 C 实现的，所以执行效率会比较高，Python中使用lxml中的etree模块来实现。

正则表达式就是编程语言中一种语法，每种编程语言基本上都有正则表达式，使用正则能解决日常工作中遇到的绝大部分关于字符串处理的问题，非常的实用，正则用在网络爬虫的话主要是把html看成是一张平铺的文本，然后在文本中匹配需要的信息，Python中的正则也是用C实现的所以效率也是很高，Python中是正则使用内置库re来实现。

上面介绍的三种方法可以相互替代也可以结合使用，三种方法都有自己的语法，当然也有相同的地方，基本上懂前端的话前两种匹配方式还是比较容易上手的，正则的话Tatsumi觉得也是非常的重要的，熟练掌握正则能让工作效率提高，项目实现也会变得更灵活。


3.数据的存储
从网络上抓取下来的信息要做好存储，方便之后的分析与挖掘时对数据的重新调用。

对于数据量的不同有不同的存储方式。少量的数据可以使用Json格式或者CSV文件进行保存。

Json本质上就时一个字符串，类似与Python的字典格式，Json格式的优点在于易于人们阅读和编写，同时也易于机器解析和生成，所以Json更多的使用在网络服务器中传输与交换信息当中，要查看一个Json数据，可以把数据复制粘贴到 http://www.json.cn/ 进行格式化，这样子能更好的看清楚Json的格式。

CSV文件的话大家应该都会比较熟悉，其实就是以一张二维表的方式存储数据，行数每一条信息的记录，列是信息的字段或者成为属性。CSV可以用excel来打开观察，一般的CSV文件都是以逗号分隔符来分隔的。

如果爬取到的数据量比较大，或者需要把数据分开保存，调用的使用又需要联合使用，那么就有必要使用的数据库来存储。主流的数据库一般有两种，一种的关系型数据库（代表是Oracle,Mysql），一种时非关系型数据库（MongoDB）

其中Oracle主要是企业级别使用的数据库，个人的话使用Mysql的Community版本就足够了，Mysql的数据结构存储也是二维表的形式，可以理解为是多张CSV或EXCEL的集合，Mysql相比CSV的优势在于查询和表链接会更加方便。

MongoDB这种非关系型数据库就不存在表链接，他像是一种拓展型的Json格式，一行记录代表一条信息，但记录里可以有多层的类似与字典的格式来保存信系，所以MongoDB一般用来存放非结构化的数据，功能上与Mysql互补，使用起来灵活度也更高。



4.多线程提速
掌握了前面3步骤基本上就能实现简单的静态爬虫了。为了提高爬虫效率，充分发挥计算机的性能，可以使用多线程技术来协助爬虫。

多线程的意思就类似与初中我们学习物理时的并联概念，多核处理器用不同的核去并行的执行代码。而单线程就类似于串联，只用一个核去执行代码所以效率会比较低。当然，也不是所有的程序都适合使用多线程，一般时关于IO（输入输出流）网络请求等情景多线程的优势才能更好的体现，在内存计算等数据运算领域多线程优势其实不太明显，还有可能会出现个各种问题（这里就涉及到锁，队列，变量共享等问题）。



5.异步加载-AJAX
异步加载是现在网络上最常见的加载数据的方式。所谓的异步加载就是在开始网页中不会展示全部的内容信息（可能是为了保密，也有可能是不一次加载太多减轻服务器加载的负担），而是在用户进行一些操作后，网页才会展示额外的信息（像是点击加载更到的的按钮或拉动浏览器右手边的滚动条等等）。其实实现异步加载的本质也还是数据请求与数据交换。当你点击加载更多的按钮时，你的浏览器会向服务器发送一个新请求，服务器接收到你的请求后会通过Json的格式包装好数据返回给浏览器，浏览器拿到信息后会结合Javascript技术把Json里面的数据渲染到网页中，这就是完整的异步加载流程。



值得注意的是现在一般的网络数据交互的都是Json格式而不是XML格式，所以不要被AJAX这个名字给骗了。

在爬取需要动态加载的数据时一般有两种方法：

使用requests库找到动态加载数据请求的url，并向url发送请求，然后获取服务器返回的Json数据。这种方式破解难度比较大，也比较容易被浏览器识别出我们的爬虫，然后限制我们的访问。
使用Selenium+chromedriver的方式通过Python操作一个浏览器。Selenium是一个测试用的库，可以很方便的模拟一个正常游客操作浏览器，chromedriver是一个轻量级的chrome浏览器。结合使用这2者就能方便的获取异步加载的数据，这种方式也不容易被服务器识别为一个爬虫程序，唯一的缺点是这种方式由于要控制一个浏览器所以效率会比较低。（也可以通过Selenium+chromedriver获取到网页的html然后再使用lxml的xtree模块进行解析，这样的解析效率就会有所提高）
6.Scrapy框架
一个爬虫一般有很多事情都是重复的。比如一般都有发送请求、数据解析、数据存储、使用代理、设置请求头等等。这些东西都是常规操作，基本上每个爬虫都会用到，因此如果每次都要重零开始写的话会比较浪费时间，而Scrapy就是一个把这些东西都封装好的框架，使用Scrapy框架写爬虫可以变得更高效。



这是是Scrapy框架的完整流程（按照数字的顺序来看）:

Spider首先会把向网站的发送的Request首先发送给Engine
Engine再把Request发送给Schduler进行调度
Schduler再把调度后的Request返回给Engine
Engine再次拿到请求后把请求通过一系列middleware发送给Downloader来去真正下载页面
下载后，Downloader把下载完的信息通过一系列middleware又返回个Engine
Engine把信息通过一系列middleware再发送给Spider进行页面解析
Spider拿到需要的数据后也通过一系列middleware又把数据返回给Engine
Engine最后把需要的数据打包成item发送给ItemPipeline进行下保存（可以是本地文件也可以是数据库）
其中：

Engine（引擎）：Scrapy框架的核心部分。负责在Spider和ItemPipeline、Downloader、Scheduler中间通信、传递数据等。
Spider（爬虫）：发送需要爬取的链接给Engine，最后Engine把其他模块请求回来的数据再发送给爬虫，爬虫就去解析想要的数据。这个部分是我们开发者自己写的，因为要爬取哪些链接，页面中的哪些数据是我们需要的，都是由程序员自己决定。
Scheduler（调度器）：负责接收Engine发送过来的请求，并按照一定的方式进行排列和整理，负责调度请求的顺序等。
Downloader（下载器）：负责接收Engine传过来的下载请求，然后去网络上下载对应的数据再交还给Engine。
Item Pipeline（管道）：负责将Spider（爬虫）传递过来的数据进行保存。具体保存在哪里，应该看开发者自己的需求（可以是本地文件也可以是数据库）
Downloader Middlewares（下载中间件）：可以扩展下载器和Engine之间通信功能的中间件（一般用于设置随机请求头和代理）。
Spider Middlewares（Spider中间件）：可以扩展Engine和爬虫之间通信功能的中间件。
结束啦
到这里的话基本上就是网络爬虫一个比较完整的知识点总结和概述，总的来说爬虫还是一个比较好玩的东西，大家感兴趣的话自己也可以尝试再网络中搜集一些数据来做一些简单的分析，自给自足丰衣足食也是很差不错的。

个人爬虫小项目：

1.  爬取京东Iphonex评论数据

2.  使用Scrapy对简书全站的用户关注信息进行递归爬取，并保存至Mysql