文本挖掘的那些事
分类：文章 ? 作者：温毛 ? 2018-03-06 20:23:50
文本挖掘，相信很多同学在数据科学领域或多或少都会有接触到，它跟传统的结构化数据有点不一样，需要我们对文本数据做一定的预处理才能正常使用，今天Tatsumi打算给大家讲讲一般文本挖掘的完整流程，包括分词、词型的归一化（一般针对英文文本）、去停用词、生成高质量的词字典、词列表的特征化、模型构建、可视化展示。希望同学看完后会有一个完整直观的思路和认识，废话不多说直接入正题：

1.分词：

当拿到文本数据后，无论是中文还是英文文本，都是首先对数据进行分词。相比于英文天生的每个词都是以空格作为分隔符，中文文本分词却要麻烦的多，市面上有很多做中文分词的第三方收费软件，它们分词的准确率也是相当有保证，因为通常这种第三方分词软件后台都会维护一个巨大的中文词典以保证分词效果。一般如果我们不是对精度要求太高，通常中文分词都会使用jieba库，使用jieba也是很容易实现分词的，分完词后我们就会得到一个词列表，

例如：【欢迎 各位 朋友 来到 Tatsumi 个人 博客】

2.词型的归一化：

词性的归一化一般是针对英文文本做的处理，由于英文有大小写、词态、单复数等形式的转换，但是不同的形式表示对应于实际含义都是相同的，所以在英文文本的预处理中需要把所有词性进行归一化，转化成最原始的表达形式，例如

【apples ,Apple -> apple】和【done does doing -> do】等等

3.去停用词：

这里所谓的停用的实际上是指没有特别含义的词语和符号，一般而言，如果要做情感分析的话形容词会比较重要，如果要做的是主题抽取和文本推荐，动词和名词会比较重要。还是以这句话为例：

【欢迎 各位 朋友 来到 Tatsumi 个人 博客】-> 【欢迎  朋友 来到 Tatsumi  博客】

去掉【各位、个人】这2个词对于整句话的理解没有产生变化，那么这2个词就可以剔除掉。网上有很多中英文的停用表，大家直接在网上下载就可以了



停用词表也可以不止选用一份，也可以结合多张停用词表使用，当然自己也可以制作自己的停用词表。

4.生成高质量的词字典：

以为去掉停用词就完成了所有的数据预处理操作，那你就大错特错了，所谓Garbage in, Garbage out，要保证之后做的分析或模型效果，首先就要生成一个高质量的词字典。这里生成词字典的方法主要有两种

a.只保留在所欲文本中出现频率最高的前X个词进入最终词字典。
b.只保留TF-IDF值最高的前X个词进入最终的词字典（TF-IDF下文会介绍到）
生成高质量词字典的原因主要是避免个别词语的出现会影响模型的整体效果、当然还有就是能大大降低模型的训练维度，减少计算量。

5.词列表的特征化：

文本挖掘可以算是数据科学领域里一个比较特殊的模块，因为一般文本数据都是以字符集的形式来表示，但是计算机却不认识字符集，所以要实现文本挖掘首先要做的是把手中的字符集转化为计算机认识的数值向量矩阵。

文本转化为向量的形式一般有三种：

a.Bagofwords（词袋模型）：词袋模型的含义是，首先把词字典里所有出现的词语都都各自作为一个维度指标，词字典中有X个词，最终的向量矩阵就有X个字段。然后遍历每条文本中的每个词语，词语在文本中出现的频率就作为对应该字段下的值。


b.TF-IDF：其中
TF 衡量一个词语在文档中出现的频率，计算公式为：（该词语在文档中出现的次数）/（文档中该词语的总数）

IDF 衡量一个词语有多重要，其实就是一个修正权重，计算公式为：（log_e(文档总数/含有该词语的文档总数)）

首先把词字典里所有出现的词语都各自作为一个维度指标，词字典中有X个词，最终的向量矩阵就有X个字段。然后遍历每条文本中的每个词语，计算每个词语的TF-IDF值，词语的TF-IDF值就作为对应该字段下的值。



以上a，b两种方法最基本的用法是把每个词语都看作一个唯一的单元来进行计算（统计词频或计算TF-IDF），但是只考虑单个词语会使文本丧失词与词之间的关系度量、因此可以考虑把两个词或三个词同时连续出现在一起的情况也看作一个单元来计算（统计词频或计算TF-IDF），也就是所谓的n-gram模型。

一般来说，n取越大，最后的出来的矩阵就会越系数，所以n的取值还是要根据不同的情况选择，譬如在做简单的舆情分析且数据量不算太的情况下n取2或3就大概可以了；但在做翻译系统或模拟写作、写代码等词语顺序要求非常高的需求时，n需取至5以后，效果才会比较乐观。

c.Word2Vec: Word2Vec故名思意就是把词转化为向量，其实是Word2Vec的实现原理就是一个神经网络，按一定大小的窗口移动方式，选定若干个词作为输入节点传到神经网络当中，中间用隐层的若干神经元保存词向量的最终参数，而要预测的词作为神经网络的输出节点。一般Word2Vec有两种实现形式：
CBOW：使用左侧窗口和右侧窗口预测中间目标词（无隐层、投影层简化为求和、直接使用低维向量、层次Softmax（使用哈夫曼树）或是使用负例采样）数据量不大时推荐使用

Skip-gram：使用中间词预测左右两边的目标词，数据量超大时推荐使用



          使用Word2Vec后，词字典中的每个词都对应一个高纬度的向量（维度自己定），并且会出现一个惊人的效果，就是文档里经常伴随一同出现的词，他们余弦距离也会比较接近，在现实中可以达到相当于是近义词的效果，可以方便的计算每个词之间的相似度，基于此特性，Word2Vec还能用在推荐系统上，有机会Tatsumi会再给大家介绍。



          使用Word2Vec转化为矩阵的方式是，首先把词字典里所有出现的词语都各映射成n为向量，然后遍历每条文本中的每个词语，累加每个词语的向量再除上文本中词语的数目，作为最终该文本的向量，最后对个文本重复此操作便可得到一个完整的数值矩阵。



d.Doc2Vec：与Word2Vec相似，Doc2Vec可以把这个句子或段落转化成多维向量。
5.模型构建

当之前所有的所有步骤的成功后，我们就会拿到一个全是数值的矩阵，无论你是使用Bag-of-Words、TF-IDF还是Word2Vec、Doc2Vec。有了数值型的矩阵我们就可以把数据喂给计算器实现我们的模型算法了。

关于文本挖掘的模型主要有两种，无监督的有LDA主题模型（可以抽取文本中的若干个词来表示主题）和Word2Vec（用在推荐系统计算相似度上）。有监督的有我们非常熟悉的LR、DT、NB、RF、GDBT、XGBOOT（主要用在情感分析和垃圾邮件判断等情景）还有直接可以做分类的FastText（就是一个结合了Word2Vec和Softmax的分类算法）。另外还有比较高大上的有深度学习已经强化学习方面的应用（像是机器翻译和自动聊天机器人等等）

6.可视化展示

要说文本就经典的可视化，那一定非词云莫属了。在Python中事项词云也是非常简单，输入背景图、定义好字体大小、选择衡量字体大小的指标（可以是词频也可以是TF-IDF值）就ok了


以上大概就是一个完整的文本挖掘的具体流程，接下来Tatsumi也会做一些文本挖掘的小项目，然后展示给大家，希望大家多多支持啦~！

欢迎大家交流学习

QQ：745447374

邮箱：745447374@qq.com